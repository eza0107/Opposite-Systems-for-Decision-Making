{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# parameters \n",
    "S = 100\n",
    "T = 80\n",
    "alpha = 0.4\n",
    "gamma = 0.8\n",
    "epsilon = 0.4\n",
    "k_plus = 0.0\n",
    "k_minus = 0.0\n",
    "\n",
    "# Q-tables will be of size 3 by 2\n",
    "# 3 states each having 2 actions\n",
    "def choose_action (Q_plus, Q_minus, eps, curr_state):\n",
    "    q_plus = np.array(Q_plus)\n",
    "    q_minus = np.array(Q_minus)\n",
    "    summ = (np.array(q_plus[curr_state,:]) + np.array(q_minus[curr_state,:]))/2\n",
    "    diff = (np.array(q_plus[curr_state,:]) - np.array(q_minus[curr_state,:]))\n",
    "    U = summ + np.random.uniform(-0.5,0.5)*diff\n",
    "    # print(U)\n",
    "    if np.random.uniform(0,1) < eps:\n",
    "        action = np.random.randint(0,2)\n",
    "    else:\n",
    "        action = np.argmax(U[:])\n",
    "    return action\n",
    "\n",
    "# updates the Q-values\n",
    "def update (curr_action, curr_state, Q, next_action, next_state, r,k,Q_type):\n",
    "    predict = Q[curr_state, curr_action]\n",
    "    target = r + gamma*Q[next_state, next_action]\n",
    "    Q[curr_state, curr_action] = Q[curr_state, curr_action] + alpha*piecewise(target - predict, k, Q_type) \n",
    "    \n",
    "        \n",
    "# for simplicity, Q_type is a string character indicating which Q we are updating\n",
    "# Q_type = 'POS' or 'NEG'\n",
    "def piecewise(TD_error, k, Q_type):\n",
    "    if Q_type == 'POS':\n",
    "        if TD_error >= 0:\n",
    "            return (1+k)*TD_error\n",
    "        else:\n",
    "            return (1-k)*TD_error\n",
    "    elif TD_error >= 0:\n",
    "        return (1-k)*TD_error\n",
    "    else:\n",
    "        return (1+k)*TD_error\n",
    "\n",
    "# helper function for the evolving reward probabilities\n",
    "def reward_prob(T):\n",
    "    q = np.zeros((2,2,T))\n",
    "    q[0:2,0:2,0] = np.array([[0.75,0.25],\n",
    "                [0.25,0.75]])\n",
    "    for t in range(1,T):\n",
    "        q[:,:,t] = q[:,:,t-1] + np.random.normal(0,0.025,[2,2])\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                if q[i,j,t] >=1:\n",
    "                    q[i,j,t] = 1 - (q[i,j,t] - 1)\n",
    "                elif q[i,j,t]<=0:\n",
    "                    q[i,j,t] = -q[i,j,t]\n",
    "            \n",
    "    return q\n",
    "\n",
    "# function to encode the MDP structure - transition and reward\n",
    "# return reward, next_state\n",
    "def step(curr_state, curr_action, q, t):\n",
    "    reward = 0\n",
    "    next_state = curr_state\n",
    "    # transition probabilities from 1st stage to 2nd stage\n",
    "    p = np.array([[0.7, 0.3],[0.3, 0.7]])\n",
    "    if curr_state == 0:\n",
    "        if curr_action == 0:\n",
    "            if np.random.uniform(0,1) < 0.7:\n",
    "                next_state = 1\n",
    "            else:\n",
    "                next_state = 2\n",
    "        else:\n",
    "            if np.random.uniform(0,1) < 0.3:\n",
    "                next_state = 1\n",
    "            else:\n",
    "                next_state = 2\n",
    "    else:\n",
    "        next_state = 0\n",
    "        reward = q[curr_state-1,curr_action,t]\n",
    "    return reward, next_state\n",
    "\n",
    "Q = [[0,1],\n",
    "    [-1,1],\n",
    "    [0,10]]\n",
    "q = reward_prob(T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.23655378 0.92285785]\n",
      " [1.64860478 0.6023763 ]\n",
      " [0.57382482 1.5894643 ]] and [[1.23655378 0.92285785]\n",
      " [1.64860478 0.6023763 ]\n",
      " [0.57382482 1.5894643 ]]\n"
     ]
    }
   ],
   "source": [
    "# training starts here\n",
    "for s in range(S):\n",
    "    Q_plus = np.zeros((3,2))\n",
    "    Q_minus = np.zeros((3,2))\n",
    "    curr_state = 0\n",
    "    curr_action = choose_action(Q_plus, Q_minus, epsilon, curr_state)\n",
    "    for t in range(T):\n",
    "        # as per the SARSA-rule s,a,r,s',a'\n",
    "        reward, next_state = step(curr_state,curr_action, q, t)\n",
    "        next_action = choose_action(Q_plus, Q_minus, epsilon, next_state)\n",
    "        \n",
    "        # update the Q-values\n",
    "        \n",
    "        update(curr_action, curr_state, Q_plus, next_action, next_state, reward, k_plus, \"POS\")\n",
    "        update(curr_action, curr_state, Q_minus, next_action, next_state, reward, k_minus, 'NEG')\n",
    "        \n",
    "        # update action, states\n",
    "        curr_action = next_action\n",
    "        curr_state  = next_state\n",
    "print(Q_plus,\"and\", Q_minus)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
