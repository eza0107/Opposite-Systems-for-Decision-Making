{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 120)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_excel(\"StockDataPython.xlsx\", header = None)\n",
    "\n",
    "data = np.array(data)\n",
    "data = data.squeeze()\n",
    "\n",
    "S = 20000\n",
    "T = 120\n",
    "alpha = 0.5\n",
    "gamma = 0.0\n",
    "epsilon = 0.3\n",
    "k_plus = 0.9\n",
    "k_minus = 0.9\n",
    "\n",
    "for t in range(T-1):\n",
    "    data[t] = (data[t+1] - data[t])/data[t]\n",
    "\n",
    "def choose_action(Q_plus, Q_minus, eps, curr_state):\n",
    "    q_plus = np.array(Q_plus)\n",
    "    q_minus = np.array(Q_minus)\n",
    "    summ = (q_plus[curr_state,:] + q_minus[curr_state,:])/2\n",
    "    diff = q_plus[curr_state,:]  - q_minus[curr_state, :]\n",
    "    U = summ + np.random.uniform(-0.5, 0.5)*diff\n",
    "    \n",
    "    if np.random.uniform(0,1) < eps:\n",
    "        action = np.random.randint(0,2)\n",
    "    else:\n",
    "        action = np.argmax(U[:])\n",
    "    return action\n",
    "\n",
    "# updates the Q-values\n",
    "def update (curr_action, curr_state, Q, next_action, next_state, r,k,Q_type):\n",
    "    predict = Q[curr_state, curr_action]\n",
    "    target = r + gamma*Q[next_state, next_action]\n",
    "    Q[curr_state, curr_action] = Q[curr_state, curr_action] + alpha*piecewise(target - predict, k, Q_type) \n",
    "    \n",
    "        \n",
    "# for simplicity, Q_type is a string character indicating which Q we are updating\n",
    "# Q_type = 'POS' or 'NEG'\n",
    "def piecewise(TD_error, k, Q_type):\n",
    "    if Q_type == 'POS':\n",
    "        if TD_error >= 0:\n",
    "            return (1+k)*TD_error\n",
    "        else:\n",
    "            return (1-k)*TD_error\n",
    "    elif TD_error >= 0:\n",
    "        return (1-k)*TD_error\n",
    "    else:\n",
    "        return (1+k)*TD_error\n",
    "    \n",
    "# step function to take a step in the environment\n",
    "# 0 means low bet and 1 means high bet as actions\n",
    "def step(curr_action, curr_state, t, market_change):\n",
    "    reward = (50*curr_action-25)*market_change[t]\n",
    "    next_state = curr_state\n",
    "    return reward, next_state\n",
    "\n",
    "\n",
    "#---------------Start Training Here------------------\n",
    "\n",
    "#-----needed for plotting purposes only--------------\n",
    "plus = np.zeros((S,T,1,2))\n",
    "minus = np.zeros((S,T,1,2))\n",
    "Bet = np.zeros((S,T))\n",
    "update_plus = np.zeros((S,T))\n",
    "update_minus = np.zeros((S,T))\n",
    "RPE = np.zeros((S,T))\n",
    "v = np.zeros((S,T))\n",
    "#----------------------------------------------------\n",
    "\n",
    "for s in range(S):\n",
    "    Q_plus = np.zeros((1,2))\n",
    "    Q_minus = np.zeros((1,2))\n",
    "    curr_state = 0\n",
    "    curr_action = choose_action(Q_plus, Q_minus, epsilon, curr_state)\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        Bet[s,t] = 50*curr_action + 25\n",
    "        counter = Bet[s,t] - 50\n",
    "        \n",
    "        reward, next_state = step(curr_action, curr_state, t, data)\n",
    "        next_action = choose_action(Q_plus, Q_minus, epsilon, next_state)\n",
    "        v[s,t] = Bet[s,t]*data[t]\n",
    "        \n",
    "        #--- this is not part of the SARSA but specific to the Stock Market Task\n",
    "        error_plus = data[t]*counter - Q_plus[curr_state, curr_action]\n",
    "        error_minus = data[t]*counter - Q_minus[curr_state,curr_action]\n",
    "        update_plus[s,t] = piecewise(error_plus, k_plus, 'POS')\n",
    "        update_minus[s,t] = piecewise(error_minus, k_minus, 'NEG')\n",
    "        \n",
    "        # SARSA update\n",
    "        update(curr_action, curr_state, Q_plus, next_action, next_state, reward, k_plus, \"POS\")\n",
    "        update(curr_action, curr_state, Q_minus, next_action, next_state, reward, k_minus, 'NEG')\n",
    "        \n",
    "        # update action, states\n",
    "        curr_action = next_action\n",
    "        curr_state  = next_state\n",
    "        \n",
    "        # updateing RPE - Reward Prediction Error\n",
    "        if t==0:\n",
    "            RPE[s,t] = v[s,t]\n",
    "        elif t==1:\n",
    "            RPE[s,t] = v[s,t] - RPE[s,t-1]\n",
    "        else:\n",
    "            temp = RPE[s,:t].squeeze()\n",
    "            RPE[s,t] = (v[s,t] - np.mean(temp))/(np.std(temp))\n",
    "            \n",
    "    plus[s,t,:,:] = Q_plus\n",
    "    minus[s,t,:,:] = Q_minus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
